
# üìù Publications 
## üéôAffective Computing


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">MM 2025</div><img src='https://github.com/user-attachments/assets/2d3f05e6-db6a-40fa-a535-fa99e60522cf' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Generalizable Engagement Estimation in Conversation via Domain Prompting and Parallel Attention](https://arxiv.org/abs/2508.14448) \\
Yangche Yu, **Yin Chen**, Jia Li, Peng Jia, Yu Zhang, Li Dai, Zhenzhen Hu, Meng Wang, Richang Hong

[**Github**](https://github.com/MSA-LMC/DAT) <strong><span class='show_paper_citations' data='4FA6C0AAAAAJ:qjMakFHDy7sC'></span></strong> ![GitHub stars](https://img.shields.io/github/stars/MSA-LMC/DAT)

- The Winner's Solution of MM 2025 Human Engagement Estimation Challenge.

</div>
</div>
 
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">MM 2024</div><img src='images\dat.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[DAT: Dialogue-Aware Transformer with Modality-Group Fusion for Human Engagement Estimation](https://arxiv.org/pdf/2410.08470) \\
Jia Li, Yangchen Yu, **Yin Chen**, Yu Zhang, Peng Jia, Yunbo Xu, Ziqiang Li, Meng Wang, Richang Hong

[**Github**](https://github.com/MSA-LMC/DAT) <strong><span class='show_paper_citations' data='4FA6C0AAAAAJ:qjMakFHDy7sC'></span></strong> ![GitHub stars](https://img.shields.io/github/stars/MSA-LMC/DAT)

- The Winner's Solution of MM 2024  Human Engagement Estimation Challenge.

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ArXiv 2024</div><img src='images/s4d.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Static for Dynamic: Towards a Deeper Understanding of Dynamic Facial Expressions Using Static Expression Data](https://arxiv.org/pdf/2409.06154)\\
**Yin Chen‚Ä†**, Jia Li, Yu Zhang, Zhenzhen Hu, Shiguang Shan, Meng Wang, Richang Hong*
<!-- 
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/unilearn-enhancing-dynamic-facial-expression/dynamic-facial-expression-recognition-on-dfew)](https://paperswithcode.com/sota/dynamic-facial-expression-recognition-on-dfew?p=unilearn-enhancing-dynamic-facial-expression)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/unilearn-enhancing-dynamic-facial-expression/dynamic-facial-expression-recognition-on)](https://paperswithcode.com/sota/dynamic-facial-expression-recognition-on?p=unilearn-enhancing-dynamic-facial-expression)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/unilearn-enhancing-dynamic-facial-expression/dynamic-facial-expression-recognition-on-mafw)](https://paperswithcode.com/sota/dynamic-facial-expression-recognition-on-mafw?p=unilearn-enhancing-dynamic-facial-expression)
 -->


[**Github**](https://github.com/MSA-LMC/S4D) <strong><span class='show_paper_citations' data='4FA6C0AAAAAJ:qjMakFHDy7sC'></span></strong> ![GitHub stars](https://img.shields.io/github/stars/MSA-LMC/S4D)

-  The offical implementation of the paper: "Static for Dynamic: Towards a Deeper Understanding of Dynamic Facial Expressions Using Static Expression Data".

</div>
</div>



<!-- ## üéô Dynamic Facial Expression Recognition -->

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TAFFC 2024</div><img src='images/iShot_2023-12-12_10.02.57.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[From Static to Dynamic: Adapting Landmark-Aware Image Models for Facial Expression Recognition in Videos](https://arxiv.org/abs/2312.05447)\\
**Yin Chen‚Ä†**, Jia Li‚Ä†*, Shiguang Shan, Meng Wang, Richang Hong*
<!-- [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/from-static-to-dynamic-adapting-landmark-1/dynamic-facial-expression-recognition-on-dfew)](https://paperswithcode.com/sota/dynamic-facial-expression-recognition-on-dfew?p=from-static-to-dynamic-adapting-landmark-1)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/from-static-to-dynamic-adapting-landmark-1/dynamic-facial-expression-recognition-on)](https://paperswithcode.com/sota/dynamic-facial-expression-recognition-on?p=from-static-to-dynamic-adapting-landmark-1)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/from-static-to-dynamic-adapting-landmark-1/dynamic-facial-expression-recognition-on-mafw)](https://paperswithcode.com/sota/dynamic-facial-expression-recognition-on-mafw?p=from-static-to-dynamic-adapting-landmark-1) -->



[**Github**](https://github.com/MSA-LMC/S2D) <strong><span class='show_paper_citations' data='4FA6C0AAAAAJ:qjMakFHDy7sC'></span></strong> ![GitHub stars](https://img.shields.io/github/stars/MSA-LMC/S2D)

- The offical implementation of the paper: From Static to Dynamic: Adapting Landmark-Aware Image Models for Facial Expression Recognition in Videos.

</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPRW 2023</div><img src='images\9cf7ac3f5368cecc527d73fafd59eb2.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Multimodal feature extraction and fusion for emotional reaction intensity estimation and expression classification in videos with transformers](https://openaccess.thecvf.com/content/CVPR2023W/ABAW/html/Li_Multimodal_Feature_Extraction_and_Fusion_for_Emotional_Reaction_Intensity_Estimation_CVPRW_2023_paper.html) \\
Jia Li*, **Yin Chen\***, Xuesong Zhang, Jiantao Nie, Ziqiang Li, Yangchen Yu, Yan Zhang, Richang Hong‚Ä† ,Meng Wang

[**Project**](https://github.com/cyinen/CVPR2023-ABAW5-ERI) <strong><span class='show_paper_citations' data='4FA6C0AAAAAJ:qjMakFHDy7sC'></span></strong>

![GitHub stars](https://img.shields.io/github/stars/cyinen/CVPR2023-ABAW5-ERI)

- The Winner's Solution of CVPR2023-ABAW5 Emotional Reaction Intensity (ERI) Estimation Challenge.

</div>
</div>

 
